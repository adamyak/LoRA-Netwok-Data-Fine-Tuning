{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZQnwMxp_93U"
      },
      "source": [
        "# **LoRA Netwok Data Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes pandas certifi huggingface_hub dotenv scikit-learn --quiet\n",
        "!pip install torch torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dotenv import load_dotenv\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "from peft import get_peft_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaODXBAi_3Df"
      },
      "source": [
        "Authenticate, Login to Hugging Face and Device Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if HF_TOKEN is None:\n",
        "    print(\"Error: HF_TOKEN not found in .env file.  Please set it.\")\n",
        "    exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully logged in to Hugging Face Hub.\n"
          ]
        }
      ],
      "source": [
        "# Log in to Hugging Face Hub\n",
        "try:\n",
        "    login(token=HF_TOKEN)  # Log in using the token\n",
        "    print(\"Successfully logged in to Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face Hub: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS device\n"
          ]
        }
      ],
      "source": [
        "# Device Configuration\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU found. Using CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVrq9vAi_xUj"
      },
      "source": [
        "Load & Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your CSV file, handling potential encoding issues\n",
        "try:\n",
        "    df = pd.read_csv(\"Iot_device_network_logs.csv\", encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Error: Could not decode CSV using UTF-8.  Trying latin1 encoding.\")\n",
        "    df = pd.read_csv(\"Iot_device_network_logs.csv\", encoding='latin1')  # or 'ISO-8859-1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 477426 rows and 14 columns\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 477426 entries, 0 to 477425\n",
            "Data columns (total 14 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   frame.number  477426 non-null  int64  \n",
            " 1   frame.time    477426 non-null  int64  \n",
            " 2   frame.len     477426 non-null  int64  \n",
            " 3   eth.src       477426 non-null  int64  \n",
            " 4   eth.dst       477426 non-null  int64  \n",
            " 5   ip.src        477426 non-null  int64  \n",
            " 6   ip.dst        477426 non-null  int64  \n",
            " 7   ip.proto      477426 non-null  float64\n",
            " 8   ip.len        477426 non-null  float64\n",
            " 9   tcp.len       477426 non-null  float64\n",
            " 10  tcp.srcport   477426 non-null  float64\n",
            " 11  tcp.dstport   477426 non-null  float64\n",
            " 12  Value         477426 non-null  float64\n",
            " 13  normality     477426 non-null  int64  \n",
            "dtypes: float64(6), int64(8)\n",
            "memory usage: 51.0 MB\n"
          ]
        }
      ],
      "source": [
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training example:\n",
            "{'instruction': 'Determine the type of network traffic for the given log entry.', 'input': 'Source IP: 1921680121.0, Destination IP: 1921680198.0, Protocol: 1.0, Source Port: 0.0, Destination Port: 0.0, Frame Length: 98.0 bytes.', 'output': 'DDoS attack detected. High volume of traffic to a specific target.'}\n"
          ]
        }
      ],
      "source": [
        "# Mapping of traffic types\n",
        "attacks = {\n",
        "    0: \"Normal traffic between IoT devices.\",\n",
        "    1: \"Misconfiguration detected. Possibly a device sending out-of-spec data.\",\n",
        "    2: \"DDoS attack detected. High volume of traffic to a specific target.\",\n",
        "    3: \"Data type probing. Strange payloads (e.g., strings) sent to devices expecting specific types.\",\n",
        "    4: \"Network scan identified. The source is likely scanning for active devices or open ports.\",\n",
        "    5: \"Man-in-the-Middle attack detected. Suspicious interception between source and destination.\"\n",
        "}\n",
        "\n",
        "# Function to format the input and output using an instruction-based format\n",
        "def format_log(row, dataset_type):\n",
        "    instruction = \"Determine the type of network traffic for the given log entry.\"\n",
        "    input_data = (\n",
        "        f\"Source IP: {row['ip.src']}, \"\n",
        "        f\"Destination IP: {row['ip.dst']}, \"\n",
        "        f\"Protocol: {row['ip.proto']}, \"\n",
        "        f\"Source Port: {row['tcp.srcport']}, \"\n",
        "        f\"Destination Port: {row['tcp.dstport']}, \"\n",
        "        f\"Frame Length: {row['frame.len']} bytes.\"\n",
        "    )\n",
        "    output = attacks.get(int(row['normality']), \"Unknown traffic type.\")\n",
        "\n",
        "    formatted_entry = {\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": input_data,\n",
        "        \"output\": output\n",
        "    }\n",
        "\n",
        "    if dataset_type == 'train':\n",
        "        train_data.append(formatted_entry)\n",
        "    elif dataset_type == 'valid':\n",
        "        valid_data.append(formatted_entry)\n",
        "    else:  # test\n",
        "        test_data.append(formatted_entry)\n",
        "\n",
        "# Creating balanced samples by attack type\n",
        "N_PER_TYPE = 100\n",
        "samples = []\n",
        "for attack_type in range(6):\n",
        "    subset = df[df[\"normality\"] == attack_type].sample(n=N_PER_TYPE, random_state=42)\n",
        "    samples.append(subset)\n",
        "balanced_df = pd.concat(samples).sample(frac=1, random_state=42)\n",
        "\n",
        "# Splitting data into train, valid, and test\n",
        "train_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% valid, 15% test\n",
        "\n",
        "# Initialize the lists\n",
        "train_data = []\n",
        "valid_data = []\n",
        "test_data = []\n",
        "\n",
        "# Apply formatting\n",
        "for _, row in train_df.iterrows():\n",
        "    format_log(row, 'train')\n",
        "\n",
        "for _, row in valid_df.iterrows():\n",
        "    format_log(row, 'valid')\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    format_log(row, 'test')\n",
        "\n",
        "# Export (optional)\n",
        "with open(\"train_data_logs.json\", \"w\") as f:\n",
        "    json.dump(train_data, f, indent=2)\n",
        "\n",
        "with open(\"valid_data_logs.json\", \"w\") as f:\n",
        "    json.dump(valid_data, f, indent=2)\n",
        "\n",
        "with open(\"test_data_logs.json\", \"w\") as f:\n",
        "    json.dump(test_data, f, indent=2)\n",
        "\n",
        "# Display example\n",
        "print(\"Training example:\")\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLPAGv2t_tAR"
      },
      "source": [
        "Load & Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input token length statistics:\n",
            "count    420.000000\n",
            "mean      85.714286\n",
            "std       13.796658\n",
            "min       68.000000\n",
            "25%       68.000000\n",
            "50%       90.000000\n",
            "75%      101.000000\n",
            "max      103.000000\n",
            "dtype: float64\n",
            "\n",
            "Output token length statistics:\n",
            "count    420.000000\n",
            "mean      17.257143\n",
            "std        5.042594\n",
            "min        8.000000\n",
            "25%       16.000000\n",
            "50%       18.000000\n",
            "75%       21.000000\n",
            "max       24.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load sample data\n",
        "df_json = pd.read_json(\"train_data_logs.json\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Calculate lengths\n",
        "input_lengths = []\n",
        "output_lengths = []\n",
        "\n",
        "for i, row in df_json.iterrows():\n",
        "    combined_input = row[\"instruction\"] + \" \" + row[\"input\"]\n",
        "    input_tokens = tokenizer(combined_input)[\"input_ids\"]\n",
        "    output_tokens = tokenizer(row[\"output\"])[\"input_ids\"]\n",
        "    \n",
        "    input_lengths.append(len(input_tokens))\n",
        "    output_lengths.append(len(output_tokens))\n",
        "\n",
        "# Analysis of token lengths\n",
        "input_series = pd.Series(input_lengths)\n",
        "output_series = pd.Series(output_lengths)\n",
        "\n",
        "print(\"Input token length statistics:\")\n",
        "print(input_series.describe())\n",
        "\n",
        "print(\"\\nOutput token length statistics:\")\n",
        "print(output_series.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/420 [00:00<?, ? examples/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Map: 100%|██████████| 420/420 [00:00<00:00, 10454.77 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the JSON data into a Pandas DataFrame\n",
        "df_json = pd.read_json(\"train_data_logs.json\")\n",
        "dataset = Dataset.from_pandas(df_json)\n",
        "\n",
        "# Load the pre-trained tokenizer\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Ensure TinyLlama is available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define a suitable max length for inputs and outputs\n",
        "# Considering the data statistics, Set max_length to 150\n",
        "max_input_length = 150  # This accounts for instruction + input combo\n",
        "max_output_length = 150  # For the output token length, since 24 was max\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    # Combine instruction and input, and ensure consistent max_length for inputs\n",
        "    prompts = [i + \" \" + x for i, x in zip(example[\"instruction\"], example[\"input\"])]\n",
        "    inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_input_length)\n",
        "\n",
        "    # Ensure the labels follow the max length meant for output, i.e., response generation\n",
        "    labels = tokenizer(example[\"output\"], truncation=True, padding=\"max_length\", max_length=max_output_length)[\"input_ids\"]\n",
        "\n",
        "    # Convert labels, ensuring they retain the same batch length\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Map the tokenization process over the entire dataset\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 90/90 [00:00<00:00, 4809.19 examples/s]\n",
            "Map: 100%|██████████| 90/90 [00:00<00:00, 9602.10 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the validation and test data from JSON files or directly from DataFrames\n",
        "valid_data_df = pd.read_json(\"valid_data_logs.json\")\n",
        "test_data_df = pd.read_json(\"test_data_logs.json\")\n",
        "\n",
        "valid_dataset = Dataset.from_pandas(valid_data_df)\n",
        "test_dataset = Dataset.from_pandas(test_data_df)\n",
        "\n",
        "# Tokenize validation and test datasets\n",
        "valid_tokenized_dataset = valid_dataset.map(tokenize, batched=True, batch_size=16)\n",
        "test_tokenized_dataset = test_dataset.map(tokenize, batched=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-21): 22 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16  # Use float16 for reduced memory usage\n",
        ")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lora Configuartion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust based on the model\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    predictions = pred.predictions  # Raw logits or probabilities\n",
        "\n",
        "    # Check for correct shape of predictions (should be (batch_size, num_classes))\n",
        "    if len(predictions.shape) == 1:\n",
        "        print(\"Error: predictions should have shape (batch_size, num_classes).\")\n",
        "        return {}\n",
        "\n",
        "    # Convert probabilities/logits to predicted class labels\n",
        "    try:\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting predictions to class labels: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Flatten arrays if needed\n",
        "    if labels.ndim > 1:\n",
        "        labels = labels.flatten()\n",
        "    if predictions.ndim > 1:\n",
        "        predictions = predictions.flatten()\n",
        "\n",
        "    # Now check if shapes are compatible *before* calling accuracy_score\n",
        "    if labels.shape != predictions.shape:\n",
        "        print(f\"Error: Labels and predictions have incompatible shapes: {labels.shape} vs {predictions.shape}\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        acc = accuracy_score(labels, predictions)\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing accuracy: {e}\")\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing precision/recall/f1: {e}\")\n",
        "        return {}\n",
        "\n",
        "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [700/700 13:03, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>11.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>6.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.946800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.193800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.082000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.957800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.898900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.801300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.656800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.606100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.532400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.373300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.368800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.335600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.323200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.320700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.320100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.316000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.317900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.307300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.302800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.309600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.295600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.295400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.302000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.302000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.304300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.295600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.294400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.291300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.292700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.288200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.284700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.290600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.288000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.286300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=700, training_loss=0.7484269183022635, metrics={'train_runtime': 784.3245, 'train_samples_per_second': 5.355, 'train_steps_per_second': 0.892, 'total_flos': 3914714603520000.0, 'train_loss': 0.7484269183022635, 'epoch': 10.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_tinyllama_network_logs\",\n",
        "    per_device_train_batch_size=6,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    gradient_checkpointing=False,  # Disable to avoid grad_fn error\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to= \"none\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=valid_tokenized_dataset, \n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:33]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2837545573711395, 'eval_accuracy': 0.8759259259259259, 'eval_precision': 0.847046226188803, 'eval_recall': 0.8759259259259259, 'eval_f1': 0.8612413793466028, 'eval_runtime': 8.3416, 'eval_samples_per_second': 10.789, 'eval_steps_per_second': 1.439, 'epoch': 10.0}\n"
          ]
        }
      ],
      "source": [
        "# Optional: Evaluate on the validation dataset after training\n",
        "trainer.evaluate(eval_dataset=valid_tokenized_dataset)\n",
        "\n",
        "# Test evaluation (specific if you want a separate test evaluation)\n",
        "test_results = trainer.evaluate(eval_dataset=test_tokenized_dataset)\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the Finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results: {'eval_loss': 0.2837545573711395, 'eval_accuracy': 0.8759259259259259, 'eval_precision': 0.847046226188803, 'eval_recall': 0.8759259259259259, 'eval_f1': 0.8612413793466028, 'eval_runtime': 8.295, 'eval_samples_per_second': 10.85, 'eval_steps_per_second': 1.447, 'epoch': 10.0}\n",
            "Validation Results: {'eval_loss': 0.28205257654190063, 'eval_accuracy': 0.8774074074074074, 'eval_precision': 0.8523760658864911, 'eval_recall': 0.8774074074074074, 'eval_f1': 0.8647063362624825, 'eval_runtime': 8.1708, 'eval_samples_per_second': 11.015, 'eval_steps_per_second': 1.469, 'epoch': 10.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluating on the Validation and Test set\n",
        "results_finetuned_valid = trainer.evaluate(eval_dataset=valid_tokenized_dataset)\n",
        "results_finetuned_test = trainer.evaluate(eval_dataset=test_tokenized_dataset)\n",
        "print(\"Test Results:\", results_finetuned_test)\n",
        "print(\"Validation Results:\", results_finetuned_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:39]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adakansw/Documents/Clients/Verizon/Verizon/llm_finetuning/tinyllama_ntk_finetuning/my_ntk_finetuning_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results (Original Model): {'eval_loss': 13.783700942993164, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': 0.0022962962962962963, 'eval_precision': 0.1576343692351355, 'eval_recall': 0.0022962962962962963, 'eval_f1': 0.002731853187789008, 'eval_runtime': 9.7694, 'eval_samples_per_second': 9.212, 'eval_steps_per_second': 1.228}\n",
            "Test Results (Original Model): {'eval_loss': 13.77518081665039, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': 0.001851851851851852, 'eval_precision': 0.09812861903201135, 'eval_recall': 0.001851851851851852, 'eval_f1': 0.0018478689628232616, 'eval_runtime': 36.3382, 'eval_samples_per_second': 2.477, 'eval_steps_per_second': 0.33}\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained tokenizer and model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
        "original_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
        "original_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Define a Trainer \n",
        "trainer_original = Trainer(\n",
        "    model=original_model,\n",
        "    processing_class=original_tokenizer,\n",
        "    compute_metrics=compute_metrics, \n",
        ")\n",
        "\n",
        "# Evaluate the original model\n",
        "results_original_valid = trainer_original.evaluate(eval_dataset=valid_tokenized_dataset)\n",
        "results_original_test = trainer_original.evaluate(eval_dataset=test_tokenized_dataset)\n",
        "\n",
        "\n",
        "print(\"Validation Results (Original Model):\", results_original_valid)\n",
        "print(\"Test Results (Original Model):\", results_original_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy Comparison:\n",
            "\n",
            "   Dataset  Original Model  Fine-tuned Model\n",
            "Validation        0.002296          0.877407\n",
            "      Test        0.001852          0.875926\n"
          ]
        }
      ],
      "source": [
        "# ACCURACY COMPARISON BOX\n",
        "\n",
        "data = {\n",
        "    'Dataset': ['Validation', 'Test'],\n",
        "    'Original Model': [results_original_valid['eval_accuracy'], results_original_test['eval_accuracy']],\n",
        "    'Fine-tuned Model': [results_finetuned_valid['eval_accuracy'], results_finetuned_test['eval_accuracy']]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(data)\n",
        "print(\"\\nAccuracy Comparison:\\n\")\n",
        "print(comparison_df.to_string(index=False)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Original and Finetuned Trainable parameters and model size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have your model and LoRA configuration (lora_config)\n",
        "# model = get_peft_model(model, lora_config) # if you have not get_peft_model, then you should\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Size (FP16): 2098.18 MB\n",
            "Trainable Parameters Size (LoRA): 4.30 MB\n",
            "Overall Parameter Reduction: 99.80%\n"
          ]
        }
      ],
      "source": [
        "def get_model_size_in_mb(model):\n",
        "   \"\"\"Calculates the model size in megabytes.\"\"\"\n",
        "   param_size = 0\n",
        "   for param in model.parameters():\n",
        "       param_size += param.numel() * param.element_size()\n",
        "\n",
        "   buffer_size = 0\n",
        "   for buffer in model.buffers():\n",
        "       buffer_size += buffer.numel() * buffer.element_size()\n",
        "\n",
        "   size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "   return size_all_mb\n",
        "\n",
        "def get_trainable_parameters(model):\n",
        "    \"\"\"Counts the number of trainable parameters in a PyTorch model.\"\"\"\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    for param in model.parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    return total_params, trainable_params\n",
        "\n",
        "\n",
        "# Load the original full-precision model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_fp16  = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16  \n",
        ")\n",
        "model_fp16_size_mb = get_model_size_in_mb(model_fp16)\n",
        "\n",
        "\n",
        "# # Load model with quantization\n",
        "# model_int8 = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto')\n",
        "\n",
        "# # Configure LoRA\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# Apply LoRA *after* quantization\n",
        "model_lora = get_peft_model(model_fp16, lora_config)\n",
        "\n",
        "#Use  get_trainable_parameters function\n",
        "total_params, trainable_params = get_trainable_parameters(model_lora)\n",
        "trainable_params_size_mb = sum(p.numel() * p.element_size() for p in model_lora.parameters() if p.requires_grad) / 1024**2 \n",
        "\n",
        "# Calculate Overall Reduction\n",
        "reduction = (model_fp16_size_mb - trainable_params_size_mb) / model_fp16_size_mb * 100\n",
        "\n",
        "print(f\"Original Model Size (FP16): {model_fp16_size_mb:.2f} MB\")\n",
        "print(f\"Trainable Parameters Size (LoRA): {trainable_params_size_mb:.2f} MB\")\n",
        "print(f\"Overall Parameter Reduction: {reduction:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_ntk_finetuning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0861c355a632450caab30b79553d4a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2cef2a709024f83860b4094ab99cd1d",
              "IPY_MODEL_db4b19d5c0a04ee5b24ec284ecb04903",
              "IPY_MODEL_c1ea11b76acb415792ec81781d852fe1"
            ],
            "layout": "IPY_MODEL_60362e07cbb946e4832ffc6193a1004e"
          }
        },
        "0e14a21bcbb04382a66039237086c5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131ecd66fb9343a69aa4337cc4ffa88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab4ba942e5194e48b1961bf983321186",
            "placeholder": "​",
            "style": "IPY_MODEL_27fa739d88ed4fe2b6df3857346a5626",
            "value": " 500/500 [00:00&lt;00:00, 1219.73 examples/s]"
          }
        },
        "2654cfbdd3e547d086371437f77552d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27fa739d88ed4fe2b6df3857346a5626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "455450c1485f4039976fe7c0ab40463c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60362e07cbb946e4832ffc6193a1004e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99e86b0330354493a9ea1731928d7522": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6805ee4a5f7498ea176b8c15ed69448": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab4ba942e5194e48b1961bf983321186": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b27bb8dc60c74f5db75b1e6cb6544591": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4db1fbd574a48e4a6d21599c2992941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ea11b76acb415792ec81781d852fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70cb1e536b74f8c95b819e80d02f096",
            "placeholder": "​",
            "style": "IPY_MODEL_a6805ee4a5f7498ea176b8c15ed69448",
            "value": " 2/2 [01:26&lt;00:00, 40.28s/it]"
          }
        },
        "c21ce0cc664942e4b28a7ee525d8c87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de3ded3780434e0f882cd3987e10a329",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e8858199c14f2491be7483083c9e48",
            "value": "Map: 100%"
          }
        },
        "d6e8858199c14f2491be7483083c9e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da11e590cb8344c4b730d5c35778334f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4b19d5c0a04ee5b24ec284ecb04903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4db1fbd574a48e4a6d21599c2992941",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_455450c1485f4039976fe7c0ab40463c",
            "value": 2
          }
        },
        "de3ded3780434e0f882cd3987e10a329": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2cef2a709024f83860b4094ab99cd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99e86b0330354493a9ea1731928d7522",
            "placeholder": "​",
            "style": "IPY_MODEL_2654cfbdd3e547d086371437f77552d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e70cb1e536b74f8c95b819e80d02f096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5b48c8eea884899b15c9dbe245210f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c21ce0cc664942e4b28a7ee525d8c87d",
              "IPY_MODEL_fa15a115a0e34c50b89e6aa42c229e7b",
              "IPY_MODEL_131ecd66fb9343a69aa4337cc4ffa88c"
            ],
            "layout": "IPY_MODEL_da11e590cb8344c4b730d5c35778334f"
          }
        },
        "fa15a115a0e34c50b89e6aa42c229e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e14a21bcbb04382a66039237086c5ea",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b27bb8dc60c74f5db75b1e6cb6544591",
            "value": 500
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
